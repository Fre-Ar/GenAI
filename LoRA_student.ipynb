{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f1fdaf-2803-4099-b25c-fa1044854eb7",
   "metadata": {},
   "source": [
    "### Install transformers lib if not done yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3fa76-f714-4e2f-a878-b72ce286f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69adf6c2-15c8-4d87-95be-833af4499ece",
   "metadata": {},
   "source": [
    "### Import all necessary python workpackages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e505d-2715-44cd-80b4-6ce3276704a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937c0b16-1623-48ed-b009-2e7bc33178e2",
   "metadata": {},
   "source": [
    "### Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6331cd-429c-4319-b250-bb789a10a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show number of trainable parameters and model architecture\n",
    "def showModel(model):\n",
    "\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    total_size = (param_size + buffer_size) / 1024**2  # Convert to MB\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    print(f\"Fraction trainable: {trainable_params/total_params:.4f}\")\n",
    "    print(model)\n",
    "\n",
    "#LoRALayer class allow to create the low-rank neural networks to be trained\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.A = nn.Linear(in_dim, rank, bias=False)\n",
    "        self.B = nn.Linear(rank, out_dim, bias=False)\n",
    "        nn.init.normal_(self.A.weight, 0,0.01)\n",
    "        nn.init.zeros_(self.B.weight)\n",
    "        self.alpha = alpha/rank\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.alpha * (x @ self.A @ self.B)\n",
    "        x = self.B(self.A(x)) * self.alpha\n",
    "        return x\n",
    "\n",
    "#Class to build the LoRA layer along with the linear layer it \"replaces\"\n",
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "#Evaluate a model on a given data loader, calculating the accuracy as the ratio of good predictions\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct += (preds == batch[\"labels\"]).sum().item()\n",
    "            total += batch[\"labels\"].size(0)\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546dfc12-5b8a-44c8-8631-485719957ae0",
   "metadata": {},
   "source": [
    "### Setup\n",
    "1. Load a tiny bert model (pre-trained)\n",
    "2. Freeze original weights\n",
    "3. Load the sst2 dataset (we only use 500 entries split between train and test but you can change these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66f7c76-ab1e-4450-8ab4-1ce58212f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Very very small models for test, and the output will be according to two label, negative or positive sentiment\n",
    "#We load the pre-trained model including classification heads\n",
    "model_name = \"arnabdhar/tinybert-imdb\"   # small, already fine-tuned\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "      \n",
    "showModel(model)\n",
    "\n",
    "#Original weights are frozen in the model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "#Load sst2 dataset and select 5000 first samples for fast testing (you can adjust this paramater) \n",
    "#The dataset is formatted for the optimization \n",
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"train[:500]\")  # small subset for demo\n",
    "dataset = dataset.map(lambda batch: tokenizer(batch[\"sentence\"], truncation=True, padding=\"max_length\", max_length=32), batched=True)\n",
    "dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Split into train/test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f42de8-49a3-42a4-ba9c-ea9dc92c9238",
   "metadata": {},
   "source": [
    "### LoRA PETF\n",
    "1. set LoRA parameters\n",
    "2. replace linear layers by LoRA blocks\n",
    "3. set the trainable weights/parameters\n",
    "4. train the model\n",
    "5. evaluate\n",
    "6. compare it with the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8096b3bb-777e-44e0-8fb0-ee4a33e3b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA parameters, alpha used in LoRA layer is actually alpha/rank as the effective scaling factor is usuallt dependant of the rank\n",
    "#!!!TODO!! play with these parameters to see how good / long is the fine-tuning\n",
    "lora_rank = 4\n",
    "lora_alpha = 8\n",
    "\n",
    "#Replace the linear layers of the original model\n",
    "#!!! TODO !!! Check the original model to replace other layers (you can also check how much this impacts on perfromance as nothing forces you to apply LoRA for all)\n",
    "for layer in model.bert.encoder.layer:\n",
    "    layer.attention.....\n",
    "\n",
    "#We the last layer is trainable\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "showModel(model)\n",
    "\n",
    "# !!TODO ensure that LoRA layer parameters are trainable and the classifier head (see above for inspiration)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Configure the optimizer and ensures all trainable parameters will be trained\n",
    "# !!! you can change the number of epochs for testing\n",
    "criterion = nn.CrossEntropyLoss() #usual metric for classification\n",
    "optimizer = optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-4) \n",
    "num_epochs = 20\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "start_time = time.time()  # record overall start time\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Avg Loss: {running_loss/len(train_loader):.4f}\")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total training time: {total_time/60:.2f} minutes ({total_time:.1f} seconds)\")\n",
    "\n",
    "\n",
    "# !! TODO evaluate accuracy of fine-tuned model\n",
    "\n",
    "# !! TODO evaluate accuracy of based model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "base_model.to(device)\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-env",
   "language": "python",
   "name": "genai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
